{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import re\n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(line1, line2):\n",
    "    _line1 = str(line1)\n",
    "    _line2 = str(line2)\n",
    "    diff = 0\n",
    "    chars1 = len(_line1) + 10\n",
    "    chars2 = len(_line2) + 10\n",
    "    if chars1 > chars2:\n",
    "        diff = chars1 - chars2\n",
    "        if diff % 2 != 0:\n",
    "            diff = int(diff - .5)\n",
    "            _line2 = '# ' + _line2\n",
    "        else:\n",
    "            _line2 = ' ' + _line2\n",
    "        main_hashes = ''.join(['#' for i in range(chars1)])\n",
    "        hashes = ''.join(['#' for i in range(int(diff/2)+4)])\n",
    "        log_line = (f\"{main_hashes}\\n\"\n",
    "                    f\"#### {_line1} ####\\n\"\n",
    "                    f\"{hashes}{_line2} {hashes}\\n\"                    \n",
    "                    f\"{main_hashes}\\n\")\n",
    "        return(log_line)\n",
    "   \n",
    "    elif chars2 > chars1:\n",
    "        diff = chars2 - chars1\n",
    "        if diff % 2 != 0:\n",
    "            diff = int(diff - .5)\n",
    "            _line1 = '# ' + _line1\n",
    "        else:\n",
    "            _line1 = ' ' + _line1\n",
    "        main_hashes = ''.join(['#' for i in range(chars2)])\n",
    "        hashes = ''.join(['#' for i in range(int(diff/2)+4)])\n",
    "        log_line = (f\"{main_hashes}\\n\"\n",
    "                    \n",
    "                    f\"{hashes}{_line1} {hashes}\\n\"\n",
    "                    f\"#### {_line2} ####\\n\"\n",
    "                    f\"{main_hashes}\\n\")\n",
    "        return(log_line)\n",
    "        \n",
    "    else:\n",
    "        hashes = '####'\n",
    "        main_hashes = ''.join(['#' for i in range(chars2)])\n",
    "        log_line = (f\"{main_hashes}\\n\"\n",
    "                    f\"#### {_line1} ####\\n\"\n",
    "                    f\"{hashes} {_line2} {hashes}\\n\"\n",
    "                    f\"{main_hashes}\\n\")\n",
    "        return(log_line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init session \n",
    "S = requests.Session()\n",
    "\n",
    "# set URL\n",
    "URL = \"https://en.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id = '158830'\n",
    "topic = 'mineral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sections\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"pageid\": page_id,\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"sections\"\n",
    "}\n",
    "\n",
    "sectionsR = S.get(url=URL, params=PARAMS)\n",
    "sectionDATA = sectionsR.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectionDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [sec['anchor'] for sec in sectionDATA[\"parse\"][\"sections\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create section list filtering out non-data sections\n",
    "filter = [\n",
    "    #'References',\n",
    "    'See_also',\n",
    "    'External_links'\n",
    "]\n",
    "\n",
    "sec_list = [sec for sec in sectionDATA[\"parse\"][\"sections\"] if not sec['anchor'] in filter]\n",
    "\n",
    "\n",
    "len(sec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nested section_dict dictionary from section list based on toclevel\n",
    "# adding 'Z' section by hand consider refactor\n",
    "section_dict = {}\n",
    "is_new_parent = True\n",
    "parent = ''\n",
    "for i in range(len(sec_list) - 1):\n",
    "    if sec_list[i]['toclevel'] == 1 and sec_list[i + 1]['toclevel'] == 1:\n",
    "        section_dict[sec_list[i]['anchor']] = []\n",
    "    elif sec_list[i]['toclevel'] == 1 and sec_list[i + 1]['toclevel'] == 2:\n",
    "        section_dict[sec_list[i]['anchor']] = {}\n",
    "        parent = sec_list[i]['anchor']\n",
    "    elif sec_list[i]['toclevel'] == 2:\n",
    "        section_dict[parent][sec_list[i]['anchor']] = []\n",
    "section_dict['Z'] = []\n",
    "#section_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of top level sections\n",
    "secs = [sec for sec, val in section_dict.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"pageid\": page_id,\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"links\"\n",
    "}\n",
    "\n",
    "linkR = S.get(url=URL, params=PARAMS)\n",
    "linkDATA = linkR.json()\n",
    "\n",
    "pprint(linkDATA[\"parse\"][\"links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [link['*'] for link in linkDATA[\"parse\"][\"links\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links[3:4]:\n",
    "    # get page info\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": link,\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"info\",\n",
    "        'inprop': \"displaytitle\"\n",
    "    }\n",
    "\n",
    "    infoR = S.get(url=URL, params=PARAMS)\n",
    "    infoDATA = infoR.json()\n",
    "    pprint(infoDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"pageid\": page_id,\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"wikitext\"\n",
    "}\n",
    "\n",
    "textR = S.get(url=URL, params=PARAMS)\n",
    "textDATA = textR.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data\n",
    "\n",
    "lines = textDATA[\"parse\"][\"wikitext\"][\"*\"].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "\n",
    "with open(topic + '_error_log.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "    for index, line in enumerate(lines[20:1708]):\n",
    "        \n",
    "        if re.match('\\*\\[\\[', line[0:3]):\n",
    "            try:\n",
    "                item = re.search(r'\\[\\[(.*?)\\]\\]', line).group(1)\n",
    "                items.append(item)\n",
    "            except:\n",
    "                topic_error_line = f\"{topic} parse error\"\n",
    "                item_line = f\"{str(index)}, {line}\"\n",
    "                print(log_error(topic_error_line, item_line))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_infos = []\n",
    "for index, title in enumerate(items):\n",
    "    with open('info_error_log.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "        \n",
    "        url = 'https://en.wikipedia.org/w/index.php?title=' + title + '&action=info'\n",
    "\n",
    "        #print(url)\n",
    "        response = requests.get(url)\n",
    "        doc = lxml.html.fromstring(response.content)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            # info_table = doc.xpath('//*[@class=\"wikitable mw-page-info\"]')[0]\n",
    "            watchers = doc.xpath('//*[@id=\"mw-pageinfo-watchers\"]')[0][1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"watchers parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            watchers = \"#error#\"\n",
    "\n",
    "        try:\n",
    "            redirects = doc.xpath('//*[contains(text(),\"redirects\")]/../../td')[1].text\n",
    "            #redirects = doc.xpath('//*[contains(text(),\"redirects\")]/parent::node()/parent::node()')\n",
    "            #redirects = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[2]/tr[9]/td')[1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"redirects parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            redirects = \"#error#\"\n",
    "             \n",
    "        try:\n",
    "            views = doc.xpath('//*[@id=\"mw-pvi-month-count\"]/td/div')[0].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"views parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            views = \"#error#\"        \n",
    "        \n",
    "        try:\n",
    "            edits = doc.xpath('//*[@id=\"mw-pageinfo-edits\"]/td')[1].text\n",
    "            #edits = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[4]/tr[5]/td')[1].text\n",
    "    \n",
    "        except:\n",
    "            topic_error_line = f\"edits parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            edits = \"#error#\"\n",
    "        \n",
    "        item_infos.append({\n",
    "            topic: title,\n",
    "            'watchers': watchers,\n",
    "            'redirects': redirects,\n",
    "            'views': views,\n",
    "            'edits': edits\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer item_infos to json file\n",
    "import json\n",
    "with open(topic + '_infos.json', 'w') as filepath:\n",
    "    json.dump(item_infos, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_items = [item['mineral'] for item in item_infos if item['views'] == '#error#']\n",
    "\n",
    "for index, title in enumerate(error_items):\n",
    "    with open('info_error_log.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "        #print(title)\n",
    "        del_idx = 0\n",
    "        for index, item in enumerate(item_infos):\n",
    "            if item[topic] == title:\n",
    "                del_idx = index\n",
    "                #del item_infos[index]\n",
    "        #print(item)\n",
    "        \n",
    "        \n",
    "        if re.match(r'\\W*', title):\n",
    "            print('%%%')\n",
    "            search = re.search(r'(\\w+)?', title).group(1)\n",
    "            print(search)\n",
    "            title = search\n",
    "        \n",
    "        print(title)\n",
    "        \n",
    "        url = 'https://en.wikipedia.org/w/index.php?title=' + title + '&action=info'\n",
    "\n",
    "        #print(url)\n",
    "        response = requests.get(url)\n",
    "        doc = lxml.html.fromstring(response.content)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  \n",
    "        try:\n",
    "            # info_table = doc.xpath('//*[@class=\"wikitable mw-page-info\"]')[0]\n",
    "            watchers = doc.xpath('//*[@id=\"mw-pageinfo-watchers\"]')[0][1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"watchers parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            watchers = \"#error#\"\n",
    "\n",
    "        try:\n",
    "            redirects = doc.xpath('//*[contains(text(),\"redirects\")]/../../td')[1].text\n",
    "            #redirects = doc.xpath('//*[contains(text(),\"redirects\")]/parent::node()/parent::node()')\n",
    "            #redirects = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[2]/tr[9]/td')[1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"redirects parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            redirects = \"#error#\"\n",
    "             \n",
    "        try:\n",
    "            views = doc.xpath('//*[@id=\"mw-pvi-month-count\"]/td/div')[0].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"views parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            views = \"#error#\"        \n",
    "        \n",
    "        try:\n",
    "            edits = doc.xpath('//*[@id=\"mw-pageinfo-edits\"]/td')[1].text\n",
    "            #edits = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[4]/tr[5]/td')[1].text\n",
    "    \n",
    "        except:\n",
    "            topic_error_line = f\"edits parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            edits = \"#error#\"\n",
    "            \n",
    "        item_infos[del_idx] = {\n",
    "            topic: title,\n",
    "            'watchers': watchers,\n",
    "            'redirects': redirects,\n",
    "            'views': views,\n",
    "            'edits': edits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mineral_infos.json', 'r') as jfile:\n",
    "    data=jfile.read()\n",
    "\n",
    "# parse file\n",
    "item_infos = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_item_infos = item_infos\n",
    "len(_item_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_infos = [{item['views']: int(item['views'].replace(',', ''))} for item in item_infos]\n",
    "for index, item in enumerate(_item_infos):\n",
    "    if item['views'] != '#error#':\n",
    "        if item['watchers'] == 'Fewer than 30 watchers':\n",
    "            item['watchers'] = '29'\n",
    "        item['views'] = int(item['views'].replace(',', ''))\n",
    "        item['watchers'] = int(item['watchers'].replace(',', ''))\n",
    "        item['edits'] = int(item['edits'].replace(',', ''))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_views = sorted(_item_infos, key=lambda k : k['views'])\n",
    "for index, item in enumerate(by_views):\n",
    "    item['view_rank'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_edits = sorted(_item_infos, key=lambda k : k['edits'])\n",
    "for index, item in enumerate(by_edits):\n",
    "    item['edit_rank'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_watchers = sorted(_item_infos, key=lambda k : k['watchers'])\n",
    "for index, item in enumerate(by_views):\n",
    "    item['watcher_rank'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in _item_infos:\n",
    "    item['total_rank'] = (item['view_rank'] + item['edit_rank'] + item['watcher_rank']) / 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = pd.DataFrame(_item_infos)\n",
    "min_df.head()\n",
    "min_df.sort_values(by=['total_rank'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = pd.DataFrame(_item_infos)\n",
    "min_df.head()\n",
    "min_df.sort_values(by=['views'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df.sort_values(by=['watchers'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df.sort_values(by=['edits'], ascending=False).head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ETSVerify Env",
   "language": "python",
   "name": "etsverify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
