{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import re\n",
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(line1, line2):\n",
    "    _line1 = str(line1)\n",
    "    _line2 = str(line2)\n",
    "    diff = 0\n",
    "    chars1 = len(_line1) + 10\n",
    "    chars2 = len(_line2) + 10\n",
    "    if chars1 > chars2:\n",
    "        diff = chars1 - chars2\n",
    "        if diff % 2 != 0:\n",
    "            diff = int(diff - .5)\n",
    "            _line2 = '# ' + _line2\n",
    "        else:\n",
    "            _line2 = ' ' + _line2\n",
    "        main_hashes = ''.join(['#' for i in range(chars1)])\n",
    "        hashes = ''.join(['#' for i in range(int(diff/2)+4)])\n",
    "        log_line = (f\"{main_hashes}\\n\"\n",
    "                    f\"#### {_line1} ####\\n\"\n",
    "                    f\"{hashes}{_line2} {hashes}\\n\"                    \n",
    "                    f\"{main_hashes}\\n\")\n",
    "        return(log_line)\n",
    "   \n",
    "    elif chars2 > chars1:\n",
    "        diff = chars2 - chars1\n",
    "        if diff % 2 != 0:\n",
    "            diff = int(diff - .5)\n",
    "            _line1 = '# ' + _line1\n",
    "        else:\n",
    "            _line1 = ' ' + _line1\n",
    "        main_hashes = ''.join(['#' for i in range(chars2)])\n",
    "        hashes = ''.join(['#' for i in range(int(diff/2)+4)])\n",
    "        log_line = (f\"{main_hashes}\\n\"\n",
    "                    \n",
    "                    f\"{hashes}{_line1} {hashes}\\n\"\n",
    "                    f\"#### {_line2} ####\\n\"\n",
    "                    f\"{main_hashes}\\n\")\n",
    "        return(log_line)\n",
    "        \n",
    "    else:\n",
    "        hashes = '####'\n",
    "        main_hashes = ''.join(['#' for i in range(chars2)])\n",
    "        log_line = (f\"{main_hashes}\\n\"\n",
    "                    f\"#### {_line1} ####\\n\"\n",
    "                    f\"{hashes} {_line2} {hashes}\\n\"\n",
    "                    f\"{main_hashes}\\n\")\n",
    "        return(log_line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init session \n",
    "S = requests.Session()\n",
    "\n",
    "# set URL\n",
    "URL = \"https://en.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_id = \"6786395\"\n",
    "topic = 'street_names'\n",
    "subtopic = 'san_francisco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sections\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"pageid\": page_id,\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"sections\"\n",
    "}\n",
    "\n",
    "sectionsR = S.get(url=URL, params=PARAMS)\n",
    "sectionDATA = sectionsR.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parse': {'title': 'List of streets in San Francisco',\n",
       "  'pageid': 6786395,\n",
       "  'sections': [{'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'Arterial thoroughfares',\n",
       "    'number': '1',\n",
       "    'index': '1',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 158,\n",
       "    'anchor': 'Arterial_thoroughfares'},\n",
       "   {'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'Commercial corridors',\n",
       "    'number': '2',\n",
       "    'index': '2',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 1793,\n",
       "    'anchor': 'Commercial_corridors'},\n",
       "   {'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'Other streets',\n",
       "    'number': '3',\n",
       "    'index': '3',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 2898,\n",
       "    'anchor': 'Other_streets'},\n",
       "   {'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'Alleyways',\n",
       "    'number': '4',\n",
       "    'index': '4',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 4691,\n",
       "    'anchor': 'Alleyways'},\n",
       "   {'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'See also',\n",
       "    'number': '5',\n",
       "    'index': '5',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 4849,\n",
       "    'anchor': 'See_also'},\n",
       "   {'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'External links',\n",
       "    'number': '6',\n",
       "    'index': '6',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 4949,\n",
       "    'anchor': 'External_links'},\n",
       "   {'toclevel': 1,\n",
       "    'level': '2',\n",
       "    'line': 'References',\n",
       "    'number': '7',\n",
       "    'index': '7',\n",
       "    'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "    'byteoffset': 5119,\n",
       "    'anchor': 'References'}]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sectionDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [sec['anchor'] for sec in sectionDATA[\"parse\"][\"sections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'toclevel': 1,\n",
       "  'level': '2',\n",
       "  'line': 'Arterial thoroughfares',\n",
       "  'number': '1',\n",
       "  'index': '1',\n",
       "  'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "  'byteoffset': 158,\n",
       "  'anchor': 'Arterial_thoroughfares'},\n",
       " {'toclevel': 1,\n",
       "  'level': '2',\n",
       "  'line': 'Commercial corridors',\n",
       "  'number': '2',\n",
       "  'index': '2',\n",
       "  'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "  'byteoffset': 1793,\n",
       "  'anchor': 'Commercial_corridors'},\n",
       " {'toclevel': 1,\n",
       "  'level': '2',\n",
       "  'line': 'Other streets',\n",
       "  'number': '3',\n",
       "  'index': '3',\n",
       "  'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "  'byteoffset': 2898,\n",
       "  'anchor': 'Other_streets'},\n",
       " {'toclevel': 1,\n",
       "  'level': '2',\n",
       "  'line': 'Alleyways',\n",
       "  'number': '4',\n",
       "  'index': '4',\n",
       "  'fromtitle': 'List_of_streets_in_San_Francisco',\n",
       "  'byteoffset': 4691,\n",
       "  'anchor': 'Alleyways'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create section list filtering out non-data sections\n",
    "filt = [\n",
    "    'References',\n",
    "    'See_also',\n",
    "    'External_links',\n",
    "    #\"Historical_names_of_Valletta's_streets_and_squares\"\n",
    "]\n",
    "\n",
    "incl = [\n",
    "    'Schemes',\n",
    "    'Tropes'\n",
    "]\n",
    "\n",
    "sec_list = [sec for sec in sectionDATA[\"parse\"][\"sections\"] if not sec['anchor'] in filt]\n",
    "# sec_list = [sec for sec in sectionDATA[\"parse\"][\"sections\"] if sec['anchor'] in incl]\n",
    "\n",
    "\n",
    "len(sec_list)\n",
    "sec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(lines):\n",
    "    print(index, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nested section_dict dictionary from section list based on toclevel\n",
    "# refactored\n",
    "section_dict = {}\n",
    "current_parent = None\n",
    "for i in range(len(sec_list)):\n",
    "    if i == 0:\n",
    "        if sec_list[i]['toclevel'] < sec_list[i + 1]['toclevel']:\n",
    "            section_dict[sec_list[i]['line']] = {}\n",
    "            current_grandparent = sec_list[i]['line']\n",
    "        else:\n",
    "            section_dict[sec_list[i]['line']] = []\n",
    "\n",
    "    elif i != len(sec_list) - 1:\n",
    "        if sec_list[i - 1]['toclevel'] < sec_list[i]['toclevel'] and sec_list[i]['toclevel'] < sec_list[i + 1]['toclevel']:\n",
    "            section_dict[current_grandparent][sec_list[i]['line']] = {}\n",
    "            current_parent = section_dict[current_grandparent][sec_list[i]['line']]\n",
    "            \n",
    "        elif sec_list[i]['toclevel'] == sec_list[i + 1]['toclevel'] or sec_list[i]['toclevel'] > sec_list[i + 1]['toclevel']:\n",
    "            if current_parent != None:\n",
    "                current_parent[sec_list[i]['line']] = []\n",
    "            else: section_dict[sec_list[i]['line']] = []\n",
    "            \n",
    "        elif sec_list[i]['toclevel'] < sec_list[i + 1]['toclevel']:\n",
    "            section_dict[current_grandparent][sec_list[i]['line']] = {}\n",
    "            current_parent = section_dict[current_grandparent][sec_list[i]['line']]\n",
    "            \n",
    "    else:\n",
    "        if sec_list[i]['toclevel'] == sec_list[i - 1]['toclevel']:\n",
    "            section_dict[sec_list[i]['line']] = []\n",
    "        else:\n",
    "            section_dict[sec_list[i]['line']] = []\n",
    "              \n",
    "\n",
    "#pprint(section_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arterial thoroughfares': [],\n",
       " 'Commercial corridors': [],\n",
       " 'Other streets': [],\n",
       " 'Alleyways': []}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard coded name for section in section dict bec inconsistent wikipedia format\n",
    "section_dict = {'San Jose': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nested section_dict dictionary from section list based on toclevel\n",
    "# adding 'Z' section by hand consider refactor\n",
    "section_dict = {}\n",
    "is_new_parent = True\n",
    "parent = ''\n",
    "for i in range(len(sec_list) - 1):\n",
    "    if sec_list[i]['toclevel'] == 1 and sec_list[i + 1]['toclevel'] == 1:\n",
    "        section_dict[sec_list[i]['anchor']] = []\n",
    "    elif sec_list[i]['toclevel'] == 1 and sec_list[i + 1]['toclevel'] == 2:\n",
    "        section_dict[sec_list[i]['anchor']] = {}\n",
    "        parent = sec_list[i]['anchor']\n",
    "    elif sec_list[i]['toclevel'] == 2:\n",
    "        section_dict[parent][sec_list[i]['anchor']] = []\n",
    "section_dict['Z'] = []\n",
    "#section_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of top level sections\n",
    "secs = [sec for sec, val in section_dict.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make section dict if no nested sections\n",
    "section_dict = {sec['anchor']:[] for sec in sec_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get links\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"pageid\": page_id,\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"links\"\n",
    "}\n",
    "\n",
    "linkR = S.get(url=URL, params=PARAMS)\n",
    "linkDATA = linkR.json()\n",
    "\n",
    "pprint(linkDATA[\"parse\"][\"links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [link['*'] for link in linkDATA[\"parse\"][\"links\"]]\n",
    "for i, link in enumerate(links):\n",
    "    print(i, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize link data\n",
    "for link in links[3:4]:\n",
    "    # get page info\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": link,\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"info\",\n",
    "        'inprop': \"displaytitle\"\n",
    "    }\n",
    "\n",
    "    infoR = S.get(url=URL, params=PARAMS)\n",
    "    infoDATA = infoR.json()\n",
    "    pprint(infoDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "PARAMS = {\n",
    "    \"action\": \"parse\",\n",
    "    \"pageid\": page_id,\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"wikitext\"\n",
    "}\n",
    "\n",
    "textR = S.get(url=URL, params=PARAMS)\n",
    "textDATA = textR.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data\n",
    "\n",
    "lines = textDATA[\"parse\"][\"wikitext\"][\"*\"].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This is a '''list of streets in San Francisco, California'''. They are grouped by type: [[arterial thoroughfares]], commercial corridors, and other streets.\n",
      "1 \n",
      "2 ==Arterial thoroughfares==\n",
      "3 * [[19th Avenue (San Francisco)|19th Avenue]] that bisects the western part of the city, extending from Interstate 280 to [[Golden Gate Park]] on the way to the [[Golden Gate Bridge]]. The section from Interstate 280 to Golden Gate Park is also designated as [[California State Route 1]].\n",
      "4 * [[California Street (San Francisco)|California Street]]\n",
      "5 * [[Fell Street]] runs from near the terminus of the [[Central Freeway]] towards [[Golden Gate Park]], turning into Lincoln Way.\n",
      "6 * [[Geary Boulevard]] splits into Geary Street and O'Farrell Street east of Gough Street.\n",
      "7 * [[Grant Avenue]]\n",
      "8 * [[Fulton Street(San Francisco)|Fulton Street]] runs along the northern length of [[Golden Gate Park]]\n",
      "9 * [[Lincoln Way (San Francisco)|Lincoln Way]] runs along the southern length of [[Golden Gate Park]]\n",
      "10 * [[Lombard Street (San Francisco)|Lombard Street]] acts as [[US 101 (CA)|US 101]] between Richardson and Van Ness Avenues\n",
      "11 * [[Market Street (San Francisco)|Market Street]]\n",
      "12 * [[Park Presidio Boulevard]] runs through the [[Richmond District, San Francisco|Richmond District]] between 14th Avenue and Funston Avenue connecting Golden Gate Park to the [[Presidio of San Francisco]], and is itself a park. This route also carries California State Route 1.\n",
      "13 * [[Portola Drive]] is the extension of [[Market Street (San Francisco)|Market Street]] into the south and western portion of San Francisco\n",
      "14 * [[Van Ness Avenue (San Francisco)|Van Ness Avenue]] acts as [[US 101 (CA)|US 101]] through the heart of San Francisco from the [[Central Freeway]] towards the northern section of the city and to the [[Golden Gate Bridge]].\n",
      "15 \n",
      "16 ==Commercial corridors==\n",
      "17 * [[24th Street (San Francisco)|24th Street]]: Between Church and Castro Street, it forms the principal commercial corridor of [[Noe Valley, San Francisco, California|Noe Valley]]. Between Mission Street and Potrero Avenue, it forms a commercial corridor in the [[Mission District, San Francisco, California|Mission District]]. There is a [[24th Street Mission (BART station)|BART station]] at 24th and Mission Streets.\n",
      "18 * [[Columbus Avenue (San Francisco)|Columbus Avenue]] runs diagonal to the prevailing grid pattern and forms the principal commercial corridor through [[North Beach, San Francisco, California|North Beach]]\n",
      "19 * [[Fillmore Street]] forms the principal commercial corridor of both [[Pacific Heights, San Francisco, California|Pacific Heights]] and the [[Fillmore District, San Francisco, California|Fillmore]]\n",
      "20 * [[Kearny Street]]\n",
      "21 * [[Mission Street]]\n",
      "22 * [[Polk Street]]\n",
      "23 * [[Stockton Street (San Francisco)|Stockton Street]]\n",
      "24 * [[Union Street (San Francisco)|Union Street]] is the principal commercial corridor of [[Cow Hollow, San Francisco, California|Cow Hollow]]\n",
      "25 \n",
      "26 ==Other streets==\n",
      "27 *[[Third Street (San Francisco)|Third Street]]\n",
      "28 *[[22nd Street (San Francisco)]]\n",
      "29 *[[49-Mile Scenic Drive]]\n",
      "30 *[[Alemany Boulevard]]\n",
      "31 *[[Broadway (San Francisco)|Broadway]]\n",
      "32 *[[The Castro, San Francisco|Castro Street]]<ref name=\"mullins\">{{cite web |author = Mullins, Jessica | date = January 4, 2017 | url = http://www.sfgate.com/bayarea/article/The-stories-behind-San-Francisco-s-street-names-6124330.php#photo-7565937 | title = Stories Behind San Francisco's Street Names | work = San Francisco Chronicle |  accessdate = January 4, 2017}}</ref>\n",
      "33 *[[Cesar Chavez Street]]  (formerly Army Street)<ref name=\"curbed 2019-05-09\">{{citeweb |author = Editors | date = May 9, 2019 | url = https://sf.curbed.com/maps/history-names-sf-san-francisco-streets-how-named  | title = Behind the Place Names of San Francisco Streets | work = Curbed San Francisco | accessdate = 2019-05-09}}</ref>\n",
      "34 *Divisadero Street\n",
      "35 *[[Don Chee Way]]\n",
      "36 *[[Embarcadero (San Francisco)|The Embarcadero]]\n",
      "37 *[[Filbert Street (San Francisco)|Filbert Street]]\n",
      "38 *Golden Gate Avenue\n",
      "39 *[[Great Highway]]\n",
      "40 *[[Haight Street]], namesake of the [[Haight-Ashbury]] district\n",
      "41 *Hayes Street\n",
      "42 *[[Junipero Serra Boulevard]]\n",
      "43 *[[John F. Kennedy Drive]] is the main East-West arterial for Golden Gate Park, beginning where it continues on from [[Fell Street]] running westward to the [[Great Highway]].\n",
      "44 *[[Lombard Street (San Francisco)|Lombard Street]], with 8 hairpin turns\n",
      "45 *[[Montgomery Street]]\n",
      "46 *[[New Montgomery Street]]\n",
      "47 *[[Octavia Boulevard]]\n",
      "48 *[[California State Route 35|Skyline Boulevard]]\n",
      "49 *[[Vermont Street (San Francisco)|Vermont Street]], with 7 hairpin turns â€“ while the honor of \"crookedest street in the world often goes to a block of Lombard Street, a section of this street is more sinuous\n",
      "50 *[[Howard Street (San Francisco)]]\n",
      "51 \n",
      "52 ==Alleyways==\n",
      "53 *[[Balmy Alley]]\n",
      "54 *[[Belden Place]]\n",
      "55 *[[Clarion Alley]]\n",
      "56 *[[Jack Kerouac Alley]]\n",
      "57 *[[Macondray Lane]]\n",
      "58 *[[Maiden Lane (San Francisco)|Maiden Lane]]\n",
      "59 \n"
     ]
    }
   ],
   "source": [
    "for index, line in enumerate(lines[:60]):\n",
    "    print(index, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$0$-3\n",
      "$0$-4\n",
      "$0$-5\n",
      "$0$-6\n",
      "$0$-7\n",
      "$0$-8\n",
      "$0$-9\n",
      "$0$-10\n",
      "$0$-11\n",
      "$0$-12\n",
      "$0$-13\n",
      "$0$-14\n",
      "$0$-17\n",
      "$0$-18\n",
      "$0$-19\n",
      "$0$-20\n",
      "$0$-21\n",
      "$0$-22\n",
      "$0$-23\n",
      "$0$-24\n",
      "$0$-27\n",
      "$0$-28\n",
      "$0$-29\n",
      "$0$-30\n",
      "$0$-31\n",
      "$0$-32\n",
      "$0$-33\n",
      "$1$-34\n",
      "$0$-35\n",
      "$0$-36\n",
      "$0$-37\n",
      "$1$-38\n",
      "$0$-39\n",
      "$0$-40\n",
      "$1$-41\n",
      "$0$-42\n",
      "$0$-43\n",
      "$0$-44\n",
      "$0$-45\n",
      "$0$-46\n",
      "$0$-47\n",
      "$0$-48\n",
      "$0$-49\n",
      "$0$-50\n",
      "$0$-53\n",
      "$0$-54\n",
      "$0$-55\n",
      "$0$-56\n",
      "$0$-57\n",
      "$0$-58\n"
     ]
    }
   ],
   "source": [
    "# parse \n",
    "#       WikiText\n",
    "# data\n",
    "\n",
    "\n",
    "debug = False\n",
    "special = True\n",
    "\n",
    "this_sec = ''\n",
    "this_item = ''\n",
    "\n",
    "\n",
    "for index, line in enumerate(lines[:60]):\n",
    "    #print(line)\n",
    "    if debug: print(\"$$-3-$$\")\n",
    "   \n",
    "    if re.match('=+\\w', line[0:5]):\n",
    "        if debug: print(\"$$-0-$$\")\n",
    "        if this_sec != None:\n",
    "            if debug: print(\"$$-1-$$\")\n",
    "\n",
    "    if re.match('==\\w', line[0:5]):\n",
    "        if debug: print(\"$$-2-$$\")\n",
    "        this_sec = line.strip().replace(\"=\", \"\")\n",
    "\n",
    "    if debug:\n",
    "        if index != 0: \n",
    "            print('$\\n' + str(index -1) + '\\n' + lines[index - 1] + '\\n' + str(index) + '\\n' + line + '\\n$')\n",
    "            print(index, re.match('\\|\\-', lines[index - 1][0:2]))\n",
    "            print(line)\n",
    "            print(index,re.match('\\!\\w+', line[0:2]))\n",
    "\n",
    "    if this_sec != None:\n",
    "        if debug: print(\"$$-3-$$\")\n",
    "        if re.match('\\*', line[0:1]):\n",
    "            #print(index)\n",
    "            if debug: print(\"$$-4-$$\")\n",
    "            if '[[' in line:\n",
    "                print(f\"$0$-{index}\")\n",
    "                this_item = re.search(r'\\*\\s*\\[\\[(.*?)\\]\\]', line).group(1)\n",
    "            else:\n",
    "                print(f\"$1$-{index}\")\n",
    "                this_item = re.search(r'\\*(.*)', line).group(1).strip()\n",
    "            section_dict[this_sec].append(this_item)\n",
    "\n",
    "                \n",
    "                   \n",
    "                    \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#section_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19th Avenue (San Francisco)|19th Avenue',\n",
       " 'California Street (San Francisco)|California Street',\n",
       " 'Fell Street',\n",
       " 'Geary Boulevard',\n",
       " 'Grant Avenue',\n",
       " 'Fulton Street(San Francisco)|Fulton Street',\n",
       " 'Lincoln Way (San Francisco)|Lincoln Way',\n",
       " 'Lombard Street (San Francisco)|Lombard Street',\n",
       " 'Market Street (San Francisco)|Market Street',\n",
       " 'Park Presidio Boulevard',\n",
       " 'Portola Drive',\n",
       " 'Van Ness Avenue (San Francisco)|Van Ness Avenue',\n",
       " '24th Street (San Francisco)|24th Street',\n",
       " 'Columbus Avenue (San Francisco)|Columbus Avenue',\n",
       " 'Fillmore Street',\n",
       " 'Kearny Street',\n",
       " 'Mission Street',\n",
       " 'Polk Street',\n",
       " 'Stockton Street (San Francisco)|Stockton Street',\n",
       " 'Union Street (San Francisco)|Union Street',\n",
       " 'Third Street (San Francisco)|Third Street',\n",
       " '22nd Street (San Francisco)',\n",
       " '49-Mile Scenic Drive',\n",
       " 'Alemany Boulevard',\n",
       " 'Broadway (San Francisco)|Broadway',\n",
       " 'The Castro, San Francisco|Castro Street',\n",
       " 'Cesar Chavez Street',\n",
       " 'Divisadero Street',\n",
       " 'Don Chee Way',\n",
       " 'Embarcadero (San Francisco)|The Embarcadero',\n",
       " 'Filbert Street (San Francisco)|Filbert Street',\n",
       " 'Golden Gate Avenue',\n",
       " 'Great Highway',\n",
       " 'Haight Street',\n",
       " 'Hayes Street',\n",
       " 'Junipero Serra Boulevard',\n",
       " 'John F. Kennedy Drive',\n",
       " 'Lombard Street (San Francisco)|Lombard Street',\n",
       " 'Montgomery Street',\n",
       " 'New Montgomery Street',\n",
       " 'Octavia Boulevard',\n",
       " 'California State Route 35|Skyline Boulevard',\n",
       " 'Vermont Street (San Francisco)|Vermont Street',\n",
       " 'Howard Street (San Francisco)',\n",
       " 'Balmy Alley',\n",
       " 'Belden Place',\n",
       " 'Clarion Alley',\n",
       " 'Jack Kerouac Alley',\n",
       " 'Macondray Lane',\n",
       " 'Maiden Lane (San Francisco)|Maiden Lane']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make 1 list\n",
    "all_items = [item for section, items in section_dict.items() for item in items]\n",
    "# make 2 lists, nested\n",
    "# all_items = [item for section, item in section_dict.items()]\n",
    "# flatten lists 2 ways\n",
    "# all_items = [lyst1 for lyst2 in all_items for lyst1 in lyst2]\n",
    "#all_items = [item for items in all_items for item in items]\n",
    "all_items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Highway',\n",
       " 'Autobahn',\n",
       " 'Auto-estrada',\n",
       " 'Autoroute',\n",
       " 'Autostrada',\n",
       " 'Autostrasse',\n",
       " 'Bypass',\n",
       " 'Expressway',\n",
       " 'Freeway',\n",
       " 'Motorway',\n",
       " 'Pike',\n",
       " 'Turnpike',\n",
       " 'Avenue',\n",
       " 'Boulevard',\n",
       " 'Parade',\n",
       " 'Road',\n",
       " 'Street',\n",
       " 'Arcade',\n",
       " 'Alley',\n",
       " 'Bay',\n",
       " 'Branch',\n",
       " 'Brook',\n",
       " 'Burg',\n",
       " 'Byway',\n",
       " 'Camp',\n",
       " 'Center',\n",
       " 'Club',\n",
       " 'Common',\n",
       " 'Corner',\n",
       " 'Course',\n",
       " 'Dale',\n",
       " 'Divide',\n",
       " 'Drive',\n",
       " 'Estate',\n",
       " 'Flat',\n",
       " 'Forge',\n",
       " 'Fork',\n",
       " 'Fort',\n",
       " 'Gardens',\n",
       " 'Gate',\n",
       " 'Gateway',\n",
       " 'Glen',\n",
       " 'Green',\n",
       " 'Grove',\n",
       " 'Harbor',\n",
       " 'Haven',\n",
       " 'Heights',\n",
       " 'Highlands',\n",
       " 'Hollow',\n",
       " 'Key',\n",
       " 'Knoll',\n",
       " 'Landing',\n",
       " 'Lane',\n",
       " 'Light',\n",
       " 'Loaf',\n",
       " 'Lock',\n",
       " 'Lodge',\n",
       " 'Manor',\n",
       " 'Meadow',\n",
       " 'Mews',\n",
       " 'Mill',\n",
       " 'Mission',\n",
       " 'Neck',\n",
       " 'Orchard',\n",
       " 'Passage',\n",
       " 'Path',\n",
       " 'Pathway',\n",
       " 'Ranch',\n",
       " 'Rapid',\n",
       " 'Rest',\n",
       " 'Route',\n",
       " 'Row',\n",
       " 'Rue',\n",
       " 'Run',\n",
       " 'Station',\n",
       " 'Terrace',\n",
       " 'Throughway',\n",
       " 'Trace',\n",
       " 'Track',\n",
       " 'Trafficway',\n",
       " 'Trail',\n",
       " 'Trailer',\n",
       " 'Union',\n",
       " 'Vale',\n",
       " 'View',\n",
       " 'Village',\n",
       " 'Ville',\n",
       " 'Vista',\n",
       " 'Walk',\n",
       " 'Wall',\n",
       " 'Way',\n",
       " 'Well',\n",
       " 'Wynd',\n",
       " 'Cul-de-sac',\n",
       " 'Close',\n",
       " 'Court',\n",
       " 'Place',\n",
       " 'Cove',\n",
       " 'Bend',\n",
       " 'Circle',\n",
       " 'Crescent',\n",
       " 'Diagonal',\n",
       " 'Loop',\n",
       " 'Oval',\n",
       " 'Quadrant',\n",
       " 'Radial',\n",
       " 'Square',\n",
       " 'Bayou',\n",
       " 'Beach',\n",
       " 'Bluff',\n",
       " 'Bottom',\n",
       " 'Canyon',\n",
       " 'Cape',\n",
       " 'Cay',\n",
       " 'Causeway',\n",
       " 'Cliff',\n",
       " 'Creek',\n",
       " 'Crest',\n",
       " 'Curve',\n",
       " 'Fall',\n",
       " 'Field',\n",
       " 'Ford',\n",
       " 'Forest',\n",
       " 'Grade',\n",
       " 'Hill',\n",
       " 'Inlet',\n",
       " 'Island',\n",
       " 'Isle',\n",
       " 'Lake',\n",
       " 'Land',\n",
       " 'Mount',\n",
       " 'Mountain',\n",
       " 'Park',\n",
       " 'Parkway',\n",
       " 'Pass',\n",
       " 'Pine',\n",
       " 'Plain',\n",
       " 'Point',\n",
       " 'Prairie',\n",
       " 'Ridge',\n",
       " 'River',\n",
       " 'Shoal',\n",
       " 'Shore',\n",
       " 'Spring',\n",
       " 'Stream',\n",
       " 'Summit',\n",
       " 'Valley',\n",
       " 'Annex',\n",
       " 'Approach',\n",
       " 'Bridge',\n",
       " 'Bypass',\n",
       " 'Crossing',\n",
       " 'Crossroad',\n",
       " 'Dam',\n",
       " 'Esplanade',\n",
       " 'Extension',\n",
       " 'Ferry',\n",
       " 'Frontage road',\n",
       " 'Junction',\n",
       " 'Mall',\n",
       " 'Overpass',\n",
       " 'Parade',\n",
       " 'Park',\n",
       " 'Plaza',\n",
       " 'Port',\n",
       " 'Promenade',\n",
       " 'Quay',\n",
       " 'Ramp',\n",
       " 'Skyway',\n",
       " 'Spur',\n",
       " 'Stravenue',\n",
       " 'Tunnel',\n",
       " 'Underpass',\n",
       " 'Viaduct',\n",
       " '']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"../output/street_type_designations.txt\", 'r+', encoding=\"utf-8\") as filepath:\n",
    "    doc = filepath.read()\n",
    "street_types = doc.split('\\n')\n",
    "street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = []\n",
    "for section, items in section_dict.items():\n",
    "    for index, item in enumerate(items):\n",
    "        #print(item)\n",
    "        item = re.sub(r'\\[|\\]', '', item)\n",
    "        if '<ref' in item:\n",
    "            #print(item)\n",
    "            all_items.append(re.search(r'(.*)\\<ref', item).group(1))\n",
    "        elif '|' in item:\n",
    "            #print(item)\n",
    "            #all_items.append(re.search(r'\\|(.*)', item).group(1))\n",
    "            for ytem in item.split('|'):\n",
    "                all_items.append(ytem.strip())\n",
    "        elif 'and ' in item:\n",
    "            print(f\"### and-split {index}\")\n",
    "            if ',' in item:\n",
    "                #print(f\"comma-split{item.split(',')}\")\n",
    "                item = re.sub(r'and ', ',', item)\n",
    "                #print(f\"### comma-split {index}\")\n",
    "                for ytem in item.split(','):\n",
    "                    all_items.append(ytem.strip())\n",
    "            else:\n",
    "                #print(f\"and-split{item.split('and')}\")\n",
    "                #print(f\"### and-split {index}\")\n",
    "                for ytem in item.split('and '):\n",
    "                    all_items.append(ytem.strip())\n",
    "        elif '& ' in item:\n",
    "            for ytem in item.split('& '):\n",
    "                    all_items.append(ytem.strip())\n",
    "        elif ',' in item:\n",
    "            #print(f\"comma-split{item.split(',')}\")\n",
    "            print(f\"### comma-split {index}\")\n",
    "            for ytem in item.split(','):\n",
    "                all_items.append(ytem.strip())\n",
    "\n",
    "        else:\n",
    "            all_items.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index,item in enumerate(all_items): print(index, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19th Avenue (San Francisco)',\n",
       " '19th Avenue',\n",
       " 'California Street (San Francisco)',\n",
       " 'California Street',\n",
       " 'Fell Street',\n",
       " 'Geary Boulevard',\n",
       " 'Grant Avenue',\n",
       " 'Fulton Street(San Francisco)',\n",
       " 'Fulton Street',\n",
       " 'Lincoln Way (San Francisco)',\n",
       " 'Lincoln Way',\n",
       " 'Lombard Street (San Francisco)',\n",
       " 'Lombard Street',\n",
       " 'Market Street (San Francisco)',\n",
       " 'Market Street',\n",
       " 'Park Presidio Boulevard',\n",
       " 'Portola Drive',\n",
       " 'Van Ness Avenue (San Francisco)',\n",
       " 'Van Ness Avenue',\n",
       " '24th Street (San Francisco)',\n",
       " '24th Street',\n",
       " 'Columbus Avenue (San Francisco)',\n",
       " 'Columbus Avenue',\n",
       " 'Fillmore Street',\n",
       " 'Kearny Street',\n",
       " 'Mission Street',\n",
       " 'Polk Street',\n",
       " 'Stockton Street (San Francisco)',\n",
       " 'Stockton Street',\n",
       " 'Union Street (San Francisco)',\n",
       " 'Union Street',\n",
       " 'Third Street (San Francisco)',\n",
       " 'Third Street',\n",
       " '22nd Street (San Francisco)',\n",
       " '49-Mile Scenic Drive',\n",
       " 'Alemany Boulevard',\n",
       " 'Broadway (San Francisco)',\n",
       " 'The Castro',\n",
       " ' San Francisco',\n",
       " 'Castro Street',\n",
       " 'Cesar Chavez Street',\n",
       " 'Divisadero Street',\n",
       " 'Don Chee Way',\n",
       " 'Embarcadero (San Francisco)',\n",
       " 'The Embarcadero',\n",
       " 'Filbert Street (San Francisco)',\n",
       " 'Filbert Street',\n",
       " 'Golden Gate Avenue',\n",
       " 'Great Highway',\n",
       " 'Haight Street',\n",
       " 'Hayes Street',\n",
       " 'Junipero Serra Boulevard',\n",
       " 'John F. Kennedy Drive',\n",
       " 'Lombard Street (San Francisco)',\n",
       " 'Lombard Street',\n",
       " 'Montgomery Street',\n",
       " 'New Montgomery Street',\n",
       " 'Octavia Boulevard',\n",
       " 'California State Route 35',\n",
       " 'Skyline Boulevard',\n",
       " 'Vermont Street (San Francisco)',\n",
       " 'Vermont Street',\n",
       " 'Howard Street (San Francisco)',\n",
       " 'Balmy Alley',\n",
       " 'Belden Place',\n",
       " 'Clarion Alley',\n",
       " 'Jack Kerouac Alley',\n",
       " 'Macondray Lane',\n",
       " 'Maiden Lane (San Francisco)',\n",
       " 'Maiden Lane']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, item in enumerate(all_items):\n",
    "    if '#' in item:\n",
    "        #print(item)\n",
    "        #print([ytem for ytem in item.split('#')])\n",
    "        #print(all_items[:index])\n",
    "        #print(all_items[index + 1:])\n",
    "        all_items = all_items[:index-1] + [ytem for ytem in item.split('#')] + all_items[index + 1:]\n",
    "    if ',' in item:\n",
    "        all_items = all_items[:index-1] + [ytem for ytem in item.split(',')] + all_items[index + 1:]\n",
    "            \n",
    "all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if '#' in 'Jean Parisot de Valette#Jean De Valette Square': print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19th',\n",
       " '19th',\n",
       " 'California',\n",
       " 'California',\n",
       " 'Fell',\n",
       " 'Geary',\n",
       " 'Grant',\n",
       " 'Fulton',\n",
       " 'Fulton',\n",
       " 'Lincoln',\n",
       " 'Lincoln',\n",
       " 'Lombard',\n",
       " 'Lombard',\n",
       " 'Market',\n",
       " 'Market',\n",
       " 'Park Presidio',\n",
       " 'Portola',\n",
       " 'Van Ness',\n",
       " 'Van Ness',\n",
       " '24th',\n",
       " '24th',\n",
       " 'Columbus',\n",
       " 'Columbus',\n",
       " 'Fillmore',\n",
       " 'Kearny',\n",
       " 'Mission',\n",
       " 'Polk',\n",
       " 'Stockton',\n",
       " 'Stockton',\n",
       " 'Union',\n",
       " 'Union',\n",
       " 'Third',\n",
       " 'Third',\n",
       " '22nd',\n",
       " '49-Mile Scenic',\n",
       " 'Alemany',\n",
       " 'Broadway',\n",
       " 'The Castro',\n",
       " 'San Francisco',\n",
       " 'Castro',\n",
       " 'Cesar Chavez',\n",
       " 'Divisadero',\n",
       " 'Don Chee',\n",
       " 'Embarcadero',\n",
       " 'The Embarcadero',\n",
       " 'Filbert',\n",
       " 'Filbert',\n",
       " 'Golden Gate',\n",
       " 'Great',\n",
       " 'Haight',\n",
       " 'Hayes',\n",
       " 'Junipero Serra',\n",
       " 'John F. Kennedy',\n",
       " 'Lombard',\n",
       " 'Lombard',\n",
       " 'Montgomery',\n",
       " 'New Montgomery',\n",
       " 'Octavia',\n",
       " 'California State',\n",
       " 'Skyline',\n",
       " 'Vermont',\n",
       " 'Vermont',\n",
       " 'Howard',\n",
       " 'Balmy',\n",
       " 'Belden',\n",
       " 'Clarion',\n",
       " 'Jack Kerouac',\n",
       " 'Macondray',\n",
       " 'Maiden',\n",
       " 'Maiden']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, item in enumerate(all_items):\n",
    "    #remove digits\n",
    "    item = re.sub('\\d+$', '', item).strip()\n",
    "    # remove comments in parens\n",
    "    item = re.sub(r'\\((.*)\\)', '', item).strip()\n",
    "    # check if last word is street type\n",
    "    # sometimes both words in street types will have to revalidate when creating\n",
    "    if len(item.split(' ')) > 1:\n",
    "        last_word = item.split(' ')[-1]\n",
    "    if last_word in street_types:\n",
    "        #print(last_word)\n",
    "        #print(item)\n",
    "        all_items[index] = item.replace(last_word, '').strip()\n",
    "    else:\n",
    "        all_items[index] = item\n",
    "        \n",
    "        #print(item)\n",
    "all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19th',\n",
       " 'California',\n",
       " 'Fell',\n",
       " 'Geary',\n",
       " 'Grant',\n",
       " 'Fulton',\n",
       " 'Lincoln',\n",
       " 'Lombard',\n",
       " 'Market',\n",
       " 'Park Presidio',\n",
       " 'Portola',\n",
       " 'Van Ness',\n",
       " '24th',\n",
       " 'Columbus',\n",
       " 'Fillmore',\n",
       " 'Kearny',\n",
       " 'Mission',\n",
       " 'Polk',\n",
       " 'Stockton',\n",
       " 'Union',\n",
       " 'Third',\n",
       " '22nd',\n",
       " '49-Mile Scenic',\n",
       " 'Alemany',\n",
       " 'Broadway',\n",
       " 'The Castro',\n",
       " 'San Francisco',\n",
       " 'Castro',\n",
       " 'Cesar Chavez',\n",
       " 'Divisadero',\n",
       " 'Don Chee',\n",
       " 'Embarcadero',\n",
       " 'The Embarcadero',\n",
       " 'Filbert',\n",
       " 'Golden Gate',\n",
       " 'Great',\n",
       " 'Haight',\n",
       " 'Hayes',\n",
       " 'Junipero Serra',\n",
       " 'John F. Kennedy',\n",
       " 'Montgomery',\n",
       " 'New Montgomery',\n",
       " 'Octavia',\n",
       " 'California State',\n",
       " 'Skyline',\n",
       " 'Vermont',\n",
       " 'Howard',\n",
       " 'Balmy',\n",
       " 'Belden',\n",
       " 'Clarion',\n",
       " 'Jack Kerouac',\n",
       " 'Macondray',\n",
       " 'Maiden']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedupe_all_items = []\n",
    "for item in all_items:\n",
    "    if item not in dedupe_all_items:\n",
    "        dedupe_all_items.append(item)\n",
    "dedupe_all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19th',\n",
       " 'California',\n",
       " 'Fell',\n",
       " 'Geary',\n",
       " 'Grant',\n",
       " 'Fulton',\n",
       " 'Lincoln',\n",
       " 'Lombard',\n",
       " 'Market',\n",
       " 'Park Presidio',\n",
       " 'Portola',\n",
       " 'Van Ness',\n",
       " '24th',\n",
       " 'Columbus',\n",
       " 'Fillmore',\n",
       " 'Kearny',\n",
       " 'Mission',\n",
       " 'Polk',\n",
       " 'Stockton',\n",
       " 'Union',\n",
       " 'Third',\n",
       " '22nd',\n",
       " '49-Mile Scenic',\n",
       " 'Alemany',\n",
       " 'Broadway',\n",
       " 'The Castro',\n",
       " 'San Francisco',\n",
       " 'Castro',\n",
       " 'Cesar Chavez',\n",
       " 'Divisadero',\n",
       " 'Don Chee',\n",
       " 'Embarcadero',\n",
       " 'The Embarcadero',\n",
       " 'Filbert',\n",
       " 'Golden Gate',\n",
       " 'Great',\n",
       " 'Haight',\n",
       " 'Hayes',\n",
       " 'Junipero Serra',\n",
       " 'John F. Kennedy',\n",
       " 'Montgomery',\n",
       " 'New Montgomery',\n",
       " 'Octavia',\n",
       " 'California State',\n",
       " 'Skyline',\n",
       " 'Vermont',\n",
       " 'Howard',\n",
       " 'Balmy',\n",
       " 'Belden',\n",
       " 'Clarion',\n",
       " 'Jack Kerouac',\n",
       " 'Macondray',\n",
       " 'Maiden']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty string if caused eg 'Pellier Avenue & Court'\n",
    "dedupe_all_items = [item for item in dedupe_all_items if item != '']\n",
    "dedupe_all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 'all'\n",
    "with open(f\"../output/{topic}_{subtopic}_{limit}.txt\", 'w+', encoding=\"utf-8\") as fp:\n",
    "    for item in dedupe_all_items:\n",
    "        fp.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if items all are/begin with list of links [[link]]\n",
    "\n",
    "items = []\n",
    "descriptions = []\n",
    "with open(topic + '_error_log.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "    for index, line in enumerate(lines[77:291]):\n",
    "        \n",
    "        if re.match('\\*\\s*\\[\\[', line[0:5]):\n",
    "            #print(\"%%%Match%%%\")\n",
    "            #print(line)\n",
    "            try:\n",
    "                line = re.sub(r'\\<ref.*?\\</ref>|\\<ref.*\\/>', '', line)\n",
    "                \n",
    "            except:\n",
    "                topic_error_line = f\"{topic} line sub parse error\"\n",
    "                item_line = f\"{str(index)}, {line}\"\n",
    "                print(log_error(topic_error_line, item_line))\n",
    "                \n",
    "            try:\n",
    "                \n",
    "                item = re.search(r'\\[\\[(.*?)\\]\\]', line).group(1)\n",
    "                \n",
    "            except:\n",
    "                topic_error_line = f\"{topic} item parse error\"\n",
    "                item_line = f\"{str(index)}, {line}\"\n",
    "                print(log_error(topic_error_line, item_line))\n",
    "                \n",
    "            try:\n",
    "                \n",
    "                description = re.search(r'\\]\\]\\:*(.*)', line).group(1)\n",
    "                \n",
    "                \n",
    "            except:\n",
    "                topic_error_line = f\"{topic} description parse error\"\n",
    "                item_line = f\"{str(index)}, {line}\"\n",
    "                print(log_error(topic_error_line, item_line))\n",
    "                \n",
    "            items.append(item)\n",
    "            descriptions.append(description)\n",
    "        else:\n",
    "            continue\n",
    "            #print(\"%%%No%%%\")\n",
    "            #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [re.sub(r'\\:wiktionary\\:', '', item) for item in items]\n",
    "items = [re.sub(r'\\|.*', '', item) for item in items]\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to find info page for each item/link and rank \n",
    "item_infos = []\n",
    "for index, title in enumerate(items):\n",
    "    with open(topic + '_error_log.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "        \n",
    "        url = 'https://en.wikipedia.org/w/index.php?title=' + title + '&action=info'\n",
    "\n",
    "        #print(url)\n",
    "        response = requests.get(url)\n",
    "        doc = lxml.html.fromstring(response.content)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            # info_table = doc.xpath('//*[@class=\"wikitable mw-page-info\"]')[0]\n",
    "            watchers = doc.xpath('//*[@id=\"mw-pageinfo-watchers\"]')[0][1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"watchers parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            watchers = \"#error#\"\n",
    "\n",
    "        try:\n",
    "            redirects = doc.xpath('//*[contains(text(),\"redirects\")]/../../td')[1].text\n",
    "            #redirects = doc.xpath('//*[contains(text(),\"redirects\")]/parent::node()/parent::node()')\n",
    "            #redirects = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[2]/tr[9]/td')[1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"redirects parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            redirects = \"#error#\"\n",
    "             \n",
    "        try:\n",
    "            views = doc.xpath('//*[@id=\"mw-pvi-month-count\"]/td/div')[0].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"views parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            views = \"#error#\"        \n",
    "        \n",
    "        try:\n",
    "            edits = doc.xpath('//*[@id=\"mw-pageinfo-edits\"]/td')[1].text\n",
    "            #edits = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[4]/tr[5]/td')[1].text\n",
    "    \n",
    "        except:\n",
    "            topic_error_line = f\"edits parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            edits = \"#error#\"\n",
    "        \n",
    "        item_infos.append({\n",
    "            topic: title,\n",
    "            'watchers': watchers,\n",
    "            'redirects': redirects,\n",
    "            'views': views,\n",
    "            'edits': edits\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer item_infos to json file\n",
    "import json\n",
    "with open(topic + '_infos.json', 'w') as filepath:\n",
    "    json.dump(item_infos, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_items = [item[topic] for item in item_infos if item['views'] == '#error#']\n",
    "\n",
    "for index, title in enumerate(error_items):\n",
    "    with open('info_error_log.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "        #print(title)\n",
    "        del_idx = 0\n",
    "        for index, item in enumerate(item_infos):\n",
    "            if item[topic] == title:\n",
    "                del_idx = index\n",
    "                #del item_infos[index]\n",
    "        #print(item)\n",
    "        \n",
    "        \n",
    "        if re.match(r'\\W*', title):\n",
    "            print('%%%')\n",
    "            search = re.search(r'(\\w+)?', title).group(1)\n",
    "            print(search)\n",
    "            title = search\n",
    "        \n",
    "        print(title)\n",
    "        \n",
    "        url = 'https://en.wikipedia.org/w/index.php?title=' + title + '&action=info'\n",
    "\n",
    "        #print(url)\n",
    "        response = requests.get(url)\n",
    "        doc = lxml.html.fromstring(response.content)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "  \n",
    "        try:\n",
    "            # info_table = doc.xpath('//*[@class=\"wikitable mw-page-info\"]')[0]\n",
    "            watchers = doc.xpath('//*[@id=\"mw-pageinfo-watchers\"]')[0][1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"watchers parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            watchers = \"#error#\"\n",
    "\n",
    "        try:\n",
    "            redirects = doc.xpath('//*[contains(text(),\"redirects\")]/../../td')[1].text\n",
    "            #redirects = doc.xpath('//*[contains(text(),\"redirects\")]/parent::node()/parent::node()')\n",
    "            #redirects = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[2]/tr[9]/td')[1].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"redirects parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            redirects = \"#error#\"\n",
    "             \n",
    "        try:\n",
    "            views = doc.xpath('//*[@id=\"mw-pvi-month-count\"]/td/div')[0].text\n",
    "            \n",
    "        except:\n",
    "            topic_error_line = f\"views parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            views = \"#error#\"        \n",
    "        \n",
    "        try:\n",
    "            edits = doc.xpath('//*[@id=\"mw-pageinfo-edits\"]/td')[1].text\n",
    "            #edits = doc.xpath('/html/body/div[3]/div[3]/div[3]/table[4]/tr[5]/td')[1].text\n",
    "    \n",
    "        except:\n",
    "            topic_error_line = f\"edits parse error\"\n",
    "            item_line = f\"{str(index)}, {title}, {url}\"\n",
    "            print(log_error(topic_error_line, item_line))\n",
    "            filepath.write(str(index) + ' : ' + title + ' : ' + url + '\\n')\n",
    "            edits = \"#error#\"\n",
    "            \n",
    "        item_infos[del_idx] = {\n",
    "            topic: title,\n",
    "            'watchers': watchers,\n",
    "            'redirects': redirects,\n",
    "            'views': views,\n",
    "            'edits': edits\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mineral_infos.json', 'r') as jfile:\n",
    "    data=jfile.read()\n",
    "\n",
    "# parse file\n",
    "item_infos = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_item_infos = item_infos\n",
    "len(_item_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_infos = [{item['views']: int(item['views'].replace(',', ''))} for item in item_infos]\n",
    "for index, item in enumerate(_item_infos):\n",
    "    if item['views'] != '#error#':\n",
    "        if item['watchers'] == 'Fewer than 30 watchers':\n",
    "            item['watchers'] = '29'\n",
    "        item['views'] = int(item['views'].replace(',', ''))\n",
    "        item['watchers'] = int(item['watchers'].replace(',', ''))\n",
    "        #item['edits'] = int(item['edits'].replace(',', ''))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_views = sorted(_item_infos, key=lambda k : k['views'], reverse=True)\n",
    "for index, item in enumerate(by_views):\n",
    "    item['view_rank'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_edits = sorted(_item_infos, key=lambda k : k['edits'], reverse=True)\n",
    "for index, item in enumerate(by_edits):\n",
    "    item['edit_rank'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_watchers = sorted(_item_infos, key=lambda k : k['watchers'], reverse=True)\n",
    "for index, item in enumerate(by_watchers):\n",
    "    item['watcher_rank'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in _item_infos:\n",
    "    #item['total_rank'] = (item['view_rank'] + item['edit_rank'] + item['watcher_rank']) / 3\n",
    "    item['total_rank'] = (item['view_rank'] + item['watcher_rank']) / 2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_total = sorted(_item_infos, key=lambda k : k['total_rank'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe = []\n",
    "for item in by_total:\n",
    "    if item[topic] not in dedupe:\n",
    "        dedupe.append(item[topic])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "limit = 44\n",
    "top_items = dedupe[start:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{topic}_{limit}.txt\", 'w+', encoding=\"utf-8\") as fp:\n",
    "    for item in top_items:\n",
    "        fp.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = pd.DataFrame(_item_infos)\n",
    "min_df.head()\n",
    "min_df.sort_values(by=['total_rank'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = pd.DataFrame(_item_infos)\n",
    "min_df.head()\n",
    "min_df.sort_values(by=['views'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df.sort_values(by=['watchers'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df.sort_values(by=['edits'], ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data\n",
    "\n",
    "lines = textDATA[\"parse\"][\"wikitext\"][\"*\"].split('\\n')[3:-10] # [::-1]\n",
    "this_sec = {}\n",
    "sec = ''\n",
    "sub_sec = ''\n",
    "with open('error_lines.txt', 'w+', encoding=\"utf-8\") as filepath:\n",
    "    for index, line in enumerate(lines):\n",
    "        if index == 23:\n",
    "            print(line)\n",
    "        if re.match('==\\w', line[0:3]):\n",
    "            sec = line.strip().replace(\"=\", \"\")\n",
    "            if len(this_sec) > 0:\n",
    "                section_dict[secs[secs.index(sec) - 1]] = this_sec\n",
    "            this_sec = {}\n",
    "        if type(section_dict[sec]).__name__ == 'list':\n",
    "            if re.match('\\*\\[\\[', line[0:3]):\n",
    "                try:\n",
    "                    poet = re.search(r'\\[\\[(.*?)\\]\\]', line).group(1)\n",
    "                    #print(poet)\n",
    "\n",
    "                except:\n",
    "                    print(\"##########################\")\n",
    "                    print(\"#### poet parse error ####\")\n",
    "                    print(poet_method, index, line)\n",
    "                    filepath.write(line + '\\n')\n",
    "                    print(line[-2:])\n",
    "                    print(\"##########################\")\n",
    "\n",
    "                date_m = re.search('\\((.*)\\)', line)\n",
    "\n",
    "                try:\n",
    "                    date = date_m.group(0)\n",
    "                except:\n",
    "                    print(\"##########################\")\n",
    "                    print(\"#### date parse error ####\")\n",
    "                    print(index, line)\n",
    "                    filepath.write(line + '\\n')\n",
    "                    date = \"\"\n",
    "                    print(\"##########################\")\n",
    "                try:\n",
    "                    # description = re.search('\\)*,*.*', line).group(1)\n",
    "                    # description = re.search('\\),+.*', line).group(1)\n",
    "                    # description = re.search('\\]\\s.*\\)(.*)', line).group(1)\n",
    "                    description = re.search('\\]\\](\\)*,*.*)', line).group(1)\n",
    "                except Exception as e:\n",
    "                    print(\"#################################\")\n",
    "                    print(\"#### description parse error ####\")\n",
    "                    print(e)\n",
    "                    print(index, line)\n",
    "                    filepath.write(line + '\\n')\n",
    "                    print(\"#################################\")\n",
    "                # print(this_sec)\n",
    "                # print(sub_sec)\n",
    "                section_dict[sec].append({\n",
    "                    'poet': poet,\n",
    "                    'date': date,\n",
    "                    'description': description\n",
    "                })\n",
    "        else:\n",
    "            #print(sec)\n",
    "            if re.match('===\\w', line[0:4]):\n",
    "                sub_sec = line.strip().replace(\"=\", \"\")\n",
    "                #print(sub_sec)\n",
    "            if line.strip().replace(\"=\", \"\") in section_dict[sec]:\n",
    "                #print(sub_sec)\n",
    "                this_sec[sub_sec] = []\n",
    "            if sub_sec != '':\n",
    "                if re.match('\\*\\[\\[', line[0:3]):\n",
    "                    try:\n",
    "                        poet = re.search(r'\\[\\[(.*?)\\]\\]', line).group(1)\n",
    "                        #print(poet)\n",
    "                    except:\n",
    "                        print(\"##########################\")\n",
    "                        print(\"#### poet parse error ####\")\n",
    "                        print(index, line)\n",
    "                        filepath.write(line + '\\n')\n",
    "                        print(line[-2:])\n",
    "                        print(\"##########################\")\n",
    "                    date_m = re.search('\\((.*)\\)', line)\n",
    "                    try:\n",
    "                        date = date_m.group(0)\n",
    "                    except Exception as e:\n",
    "                        print(\"##########################\")\n",
    "                        print(\"#### date parse error ####\")\n",
    "                        print(e)\n",
    "                        print(index, line)\n",
    "                        filepath.write(line + '\\n')\n",
    "                        date = \"\"\n",
    "                        print(\"##########################\")\n",
    "                    try:\n",
    "                        # description = re.search('\\)*,*.*', line).group(1)\n",
    "                        # description = re.search('\\),+.*', line).group(0)\n",
    "                        # description = re.search('\\]\\s.*\\)(.*)', line).group(0)\n",
    "                        description = re.search('\\]\\](\\)*,*.*)', line).group(1)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\"#################################\")\n",
    "                        print(\"#### description parse error ####\")\n",
    "                        print(e)\n",
    "                        print(index, line)\n",
    "                        filepath.write(line + '\\n')\n",
    "                        description = \"\"\n",
    "                        print(\"#################################\")\n",
    "\n",
    "                    this_sec[sub_sec].append({\n",
    "                        'poet': poet,\n",
    "                        'date': date,\n",
    "                        'description': description\n",
    "                    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ETSVerify Env",
   "language": "python",
   "name": "etsverify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
